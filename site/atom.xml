<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Paul Smith</title>
  <subtitle>Programmer, co-founder of EveryBlock</subtitle>
  <link href="http://pauladamsmith.com/atom.xml" rel="self" />
  <link href="http://pauladamsmith.com/" />
  <id>pauladamsmith.com</id>
  <updated>2013-07-23T19:00:00Z</updated>
  <author>
    <name>Paul Smith</name>
    <email>paulsmith@pobox.com</email>
  </author>
  <entry>
    <title>Public Good Software and me</title>
    <link href="/blog/2013/07/pgs.html" />
    <id>http://pauladamsmith.com/blog/2013/07/pgs.html</id>
    <updated>2013-07-23T19:00:00Z</updated>
    <content type="html">&lt;p&gt;&lt;a href=&#34;https://publicgoodsoftware.com&#34; title=&#34;Public Good Software&#34;&gt;
    &lt;img src=&#34;/images/pgs.png&#34; style=&#34;display: block; float: right; margin-left: 15px&#34; alt=&#34;PGS logo&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Software that helps civil society organizations—non-profits, NGOs,
charities—do their work should be better. It can be better. I want to help
make it better. That’s why &lt;a href=&#34;http://www.chicagogrid.com/reviews/tech/obamas-tech-team-citys-geeks-in-residence/&#34;&gt;I’ve started, along with two colleagues from the
2012 election&lt;/a&gt;, a new company, called &lt;a href=&#34;https://publicgoodsoftware.com/&#34;&gt;Public Good Software&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you survey the kind of technology that &lt;abbr title=&#34;civil society
organizations&#34;&gt;CSOs&lt;/abbr&gt; use to support their missions, it’s a sorry sight.
It’s full of complex interfaces and complicated experiences, thin layers over
old systems, aging and poorly-supported applications, and disconnected data.
Worse, the companies that develop and sell this software seem to have
stagnated—their websites often feel frozen in time from 10 years ago. There
isn’t a lot of innovation happening here.&lt;/p&gt;

&lt;p&gt;This is frustrating. These organizations are increasingly counted on to
confront our most serious challenges, like hunger, climate change,
conservation, joblessness, homelessness, affordable housing, poverty, public
health, literacy and education, and yet the technology tools they need are not
keeping up with them. Why shouldn’t people who work at CSOs expect software
every bit as good and as powerful as what they use on their smartphones
everyday?&lt;/p&gt;

&lt;p&gt;The situation is not much better if you are a supporter of these
organizations. Let’s say you give $100 a year to your local public radio
station, volunteer regularly at a community garden, and write your
congressperson on behalf of an animal rights advocacy campaign. You should be
able to keep track of all you do, and if you choose, share it with your
community. You should be able to find new opportunities that you might not
have been aware of, based on the kinds of organizations you support.  You have
a civic profile, based on how you help others, that you should be able to
claim and control.&lt;/p&gt;

&lt;p&gt;The first problem to tackle, and the first product that PGS will be developing
to help solve, is the problem of disconnected data. It’s a fundamental problem
that impacts CSOs and their supporters. Information about donors is in one
database, volunteers in another, email subscribers in a third, then there&amp;#39;s
Facebook likers and Twitter followers and you don’t know if they’re in the
other databases … Think of Mint.com, the way that service in its early days
brought sanity to your financial life. We want to connect these disparate
databases in much the same way and provide CSOs with a new, high-level view of
their data, with more complete pictures of their supporters. We’ll do this
through the use of statistical models, summaries, and visualizations that let
CSOs track how they are doing on the goals they set for themselves. This will
become a platform on which, over time, we’ll create and add new products.&lt;/p&gt;

&lt;p&gt;We aren’t setting out to reinvent the wheel. We’re not building YACRM (yet
another CRM). We’re not even aiming to replace the technology CSOs currently
use. We want to provide new tools and experiences that reflect the new needs
of these organizations and their supporters. And it will be great, modern
software: fast, a pleasure to use, designed and built for mobile devices, with
maps and geo data throughout, and ready for international users. This is what
CSOs and their supporters deserve.&lt;/p&gt;

&lt;p&gt;We decided early on that we wanted to be aligned with our customers in a way
that was sustainable, that built trust, and held us as a company accountable
to ensure that a double-bottom-line isn’t just a convenience to be discarded
when the “real” pressure (i.e., financial) builds up. At the same time, we
knew that the best way to grow the company the way we believed it should was
through traditional capital investment. That led us to become a &lt;a href=&#34;http://www.ilga.gov/legislation/BillStatus.asp?DocNum=2897&amp;amp;GAID=11&amp;amp;DocTypeID=SB&amp;amp;LegId=63455&amp;amp;SessionID=84&#34;&gt;benefit
corporation&lt;/a&gt;. &lt;a href=&#34;#fn&#34; id=&#34;fnr&#34;&gt;*&lt;/a&gt; This is new legislation,
found in a dozen or so states, and we think we’re one of the first software
startups to go that route. Essentially what this means is that we are in all
other respects like a normal for-profit company (we are a C corp under the
hood), but that we have a social mission, stated right in our corporate
by-laws (ours is roughly “to return more capital to organizations that provide
a benefit to the public”), and there are two mechanisms ensuring that the
social mission is not discarded if it becomes inconvenient. One is that there
is a board-level position called the social benefit director, whose job is to
ensure that the company is sticking to the social mission. The other is that
our fiduciary responsibility to our shareholders does not override that social
mission. This is where the rubber meets the road—you won’t see PGS suddenly
pivot to sell software to the NRA to return a few more percentage points to
our investors.&lt;/p&gt;

&lt;p&gt;All this comes at an interesting time for the public sector.  Executive
directors and supporters alike are demanding more accountability and better
ways of measuring success or failure. At the same time, demand for CSO
services is up, while capital—in the form of dollars and volunteer time—is
flat, or even declining slightly. There is a small but increasingly vocal
minority of development directors saying CSOs need to be less obsessed with
converting every dollar to program, and to find new ways to expand and be more
effective. All this leads to an increasing need for better data and analysis,
and better tools—for fundraising, communications, volunteer mobilization—that
build on it. We think there is an enormous opportunity here.&lt;/p&gt;

&lt;p&gt;So it will be fun. I’m the CTO. My co-founders &lt;a href=&#34;http://jdkunesh.com/&#34;&gt;Jason&lt;/a&gt; and &lt;a href=&#34;http://www.danratner.com/&#34;&gt;Dan&lt;/a&gt;
were director of UX and director of development, respectively, in the OFA 2012
technology department. We’ve also got two more OFA tech alums,
&lt;a href=&#34;http://www.chrisgansen.com/&#34;&gt;Chris&lt;/a&gt; and &lt;a href=&#34;http://www.aaronsalmon.com/&#34;&gt;Aaron&lt;/a&gt;, as part of the founding team. Our current
status is, talking with potential investors, meeting with a handful of CSOs
who’ve agreed to pilot the software as we build it, and making prototypes and
getting our basic infrastructure running. We’re using &lt;a href=&#34;http://golang.org/&#34;&gt;Go&lt;/a&gt; for our server
software, which is a fun language. Incidentally, it should go without saying
that we’re big believers in open source, but most of what we develop will be
available under an open source license, and I’ll write more about that in
another later post.  But I’ve already released some open source software that
was developed on PGS time, &lt;a href=&#34;http://paulsmith.github.io/gogeos/&#34;&gt;gogeos&lt;/a&gt;, a small Go library for working
with geospatial data.  We’ll be hiring software engineers soon, so if any of
this sounds interesting to you, &lt;a href=&#34;mailto:paul@publicgoodsoftware.com&#34;&gt;drop me a line&lt;/a&gt;.&lt;/p&gt;

&lt;p class=&#34;fn&#34;&gt;&lt;a id=&#34;fn&#34;&gt;*&lt;/a&gt; Not to be confused with the &lt;a
href=&#34;http://www.bcorporation.net/&#34;&gt;B Corp certification&lt;/a&gt;, which is related
but is not a corporate structure. &lt;a href=&#34;#fnr&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Announcing gogeos, a spatial data library for Go</title>
    <link href="/blog/2013/06/gogeos.html" />
    <id>http://pauladamsmith.com/blog/2013/06/gogeos.html</id>
    <updated>2013-06-12T16:00:00Z</updated>
    <content type="html">&lt;p&gt;&lt;a href=&#34;http://paulsmith.github.io/gogeos/&#34; title=&#34;gogeos&#34;&gt;
&lt;img src=&#34;/images/gogeos.png&#34; style=&#34;display: block; float: right; margin-left: 15px&#34; alt=&#34;Example of geometry processed by gogeos&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I am announcing the initial release of &lt;a href=&#34;http://paulsmith.github.io/gogeos/&#34;&gt;gogeos&lt;/a&gt;, a library for the Go
programming language. gogeos provides spatial data operations and geometric
algorithms. While it is a Go library, the hard work is done by the
&lt;a href=&#34;http://geos.osgeo.org/&#34;&gt;GEOS&lt;/a&gt; C library.&lt;/p&gt;

&lt;p&gt;The kinds of things you can do with gogeos include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;set-theoretic operations&lt;/strong&gt;, such as computing the intersection, union, or
difference of two geometries,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;topological operations&lt;/strong&gt;, such as computing buffers and convex hulls,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;binary predicates&lt;/strong&gt;, such as whether two geometries intersect or are disjoint,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;validity checking&lt;/strong&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;http://paulsmith.github.io/gogeos/#overview&#34;&gt;much more&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It also provides interoperability with other spatial data processing systems
like &lt;a href=&#34;http://postgis.org/&#34;&gt;PostGIS&lt;/a&gt; by decoding and encoding geometries as Well-Known Text
(WKT) and Well-Known Binary (WKB).&lt;/p&gt;

&lt;p&gt;I started working on gogeos because I looked at the landscape of GIS and
spatial data libraries for Go, and found it lacking. Binding to the GEOS
library with &lt;a href=&#34;http://golang.org/cmd/cgo/&#34;&gt;cgo&lt;/a&gt; was a way to get started quickly. Relying on GEOS has
its drawbacks, for instance, it creates a large binary dependency, and cgo
doesn’t allow for cross-platform compiles.&lt;/p&gt;

&lt;p&gt;In the long term, I would like to create a pure Go library that implements
functionality such as GEOS and the &lt;a href=&#34;http://www.vividsolutions.com/jts/main.htm&#34;&gt;JTS&lt;/a&gt; provide. That would allow for use
on platforms that don’t or can’t support C shared libraries, such as Google
App Engine, and make it easier for developers to get started working with it.&lt;/p&gt;

&lt;p&gt;In the meantime, I hope that gogeos enables more developers who are working
with spatial data or GIS to get involved in the Go ecosystem.&lt;/p&gt;

&lt;p&gt;gogeos is a &lt;a href=&#34;https://github.com/paulsmith/gogeos&#34;&gt;fully open-source project&lt;/a&gt;, and I welcome contributors
and feedback.&lt;/p&gt;

&lt;p&gt;—&lt;a href=&#34;https://twitter.com/paulsmith&#34;&gt;@paulsmith&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Democratic Party’s voter registration app is now free and open-source software</title>
    <link href="/blog/2013/01/dnc_voter_reg_foss.html" />
    <id>http://pauladamsmith.com/blog/2013/01/dnc_voter_reg_foss.html</id>
    <updated>2013-01-28T21:30:00Z</updated>
    <content type="html">&lt;p&gt;We (the &lt;a href=&#34;http://democrats.org/&#34;&gt;DNC&lt;/a&gt;) have &lt;a href=&#34;https://github.com/democrats/voter-registration/issues/12#issuecomment-12804999&#34;&gt;relicensed the Democratic Party’s voter registration
application&lt;/a&gt; under a standard MIT license, and accompanied the source
code with an advisory notice regarding the use of the software. I wanted to
explain why we did this.&lt;/p&gt;

&lt;p&gt;The Democratic Party initially released &lt;a href=&#34;https://github.com/democrats/voter-registration&#34;&gt;the source code to its online voter
registration app&lt;/a&gt; late last summer, with the intent of making it
available for all the standard reasons people and organizations choose when
they open-source code: so that it can be improved, so that bugs can be fixed,
so others can take it and build further new applications on top of it.&lt;/p&gt;

&lt;p&gt;However, it was quickly apparent we had a problem with the open source
community. &lt;a href=&#34;https://github.com/democrats/voter-registration/issues/12&#34;&gt;The issue was with the license&lt;/a&gt;. It contained a clause that
placed restrictions on its use. The reason this clause was included was to
address our concerns regarding the highly regulated and closely monitored
nature of voting and voter registration. We wanted to avoid a scenario where,
either inadvertently or through malice, someone set up a site based on the
code, and without following state and federal guidelines and rules, defrauded
or disenfranchised a voter. Now, regardless of our good intentions on this
matter, the fact that we had taken a standard open source license and amended
it with this restrictive clause meant that we did not pass “free and open
source” muster, with emphasis on the “free” as in “speech”.&lt;/p&gt;

&lt;p&gt;We needed a solution that addressed both the problematic license and our
concerns regarding the good-faith use of the software that protected voters. A
member of the open source community, &lt;a href=&#34;http://www.red-bean.com/kfogel/&#34;&gt;Karl Fogel&lt;/a&gt;, stepped forward
with a proposal: change the license to an unmodified standard
&lt;a href=&#34;http://opensource.org/licenses/index.html&#34;&gt;OSI&lt;/a&gt;-approved license, and include along with the source code an
advisory document that outline these legal concerns. The notice would not be
binding or otherwise modify the license and therefore terms of use; however,
like any piece of open source software, people are “free” to use it illegally,
and free to suffer the consequences if they do. The important thing is to
remind users of their responsibility to act in accordance with the law,
especially when it comes to something as precious and beseiged as our
franchise. We feel the combination of a standard FOSS license and a
non-binding advisory document expressing the intent of the copyright holder is
a way forward for political organizations to release potentially sensitive
soure code while at the same time communicating the vital issues animating and
conditioning that release.&lt;/p&gt;

&lt;p&gt;Now, some observers may not see this as remarkable. There was a bad license,
it’s been changed, what’s the fuss? I want to acknowledge the hard work across
the organization, from software engineers to lawyers, to find a way to give
back to the open source community and satisfy the concerns of both. There are
many reasons why organizations don’t release their software as open source. We
want to set an example, however small, that there are non-license ways to
state any reservations or guiding principles your organization that ordinarily
would have prevented a release. Key among these are engaging with the
community. As we have learned time and again, good solutions often originate
through trust and dialogue.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Lexing Oscar</title>
    <link href="/blog/2013/01/lexing-oscar.html" />
    <id>http://pauladamsmith.com/blog/2013/01/lexing-oscar.html</id>
    <updated>2013-01-11T04:01:00Z</updated>
    <content type="html">&lt;p&gt;For the past &lt;em&gt;n&lt;/em&gt; years, I’ve built and hosted a web app that lets my film
buff friends and me compete by guessing who will win the Academy Awards by
voting for nominees in each category. I do a new one from scratch each time.
It’s a fun diversion, but it’s also a playground for me to try out new
skills picked up in the past year or new tools or techniques I’ve been wanting
to fool around with.&lt;/p&gt;

&lt;p&gt;The first thing I need to do each time is get a list of that year’s nominees
in some machine-readable format. Being a lazy programmer, I’m not going to
type in the 100+ nominees into a spreadsheet or text file, so I wind up
writing a short throwaway script to coax some list I’ve found online into the
form I need for importing. This sort of script is the meat-and-potatoes of the
workaday programmer, the ones you whip up in a few minutes as an intermediate
step in a larger task. Ordinarily, they’re hardly worth commenting on. They
have a vanishingly short half-life, since there is rarely any generality to be
derived from them: they only work on the exact input given.&lt;/p&gt;

&lt;p&gt;This year, I wanted to try out a new way of getting the nominee list together.
Sure, for a small task like this, there’s no compelling reason not to go with
the same kind of quick throwaway script as before. But again, the point of the
Oscars app is to exercise new or different muscles.&lt;/p&gt;

&lt;p&gt;My goal was to generate a representation of the list of nominees in a format
such as CSV suitable for importing into a database. I found a source list of
nominees, formatted as follows: the name of the category is on the first line,
then a list of nominees comes next, each requiring two lines, one being the
name of the film and the other a name or list of names associated with the
nomination, all followed by a blank line, then the subsequent category starts
on the next line and we repeat. I wanted to read in and parse text formatted
like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Directing
Amour
Michael Haneke
Beasts Of The Southern Wild
Benh Zeitlin
Life Of Pi
Ang Lee
Lincoln
Steven Spielberg
Silver Linings Playbook
David O. Russell

Actor in a Leading Role
Lincoln
Daniel Day-Lewis
…
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And convert it to this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Directing,Amour,Michael Haneke
Directing,Beasts Of The Southern Wild,Benh Zeitlin
Directing,Life Of Pi,Ang Lee
Directing,Lincoln,Steven Spielberg
Directing,Silver Linings Playbook,David O. Russell
Actor in a Leading Role,Lincoln,Daniel Day-Lewis
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Normally, to scan and parse this type of input, I would write a program to
loop over each line of the input, with a number of global state variables,
keeping track of what tokens I was currently processing. In this case, I might
have global state variables indicating whether I was currently processing a
category and what the current film is, and I would have a set of if/elif/else
statements for tests of various combinations of those variables, including for
the contents of the current line (a blank line or EOF indicating the end of a
category).&lt;/p&gt;

&lt;p&gt;Each time through the loop, then, we get a line from the text and check to see
what state we’re in. While this approach is easy to get started with, it leads
to fragile code and requires a lot of mental bookkeeping. Worse, each time
through the loop, the state of where we are and what we just did is forgotten.
That accounts for the proliferation of state variables to be checked in order
to restore the state of the processing.  Think about it, we are marching
sequentially through this text, wouldn&amp;#39;t it be nice if we could just pick up
where we left off with the last action?&lt;/p&gt;

&lt;p&gt;My approach this time is inspired by &lt;a href=&#34;http://www.youtube.com/watch?v=HxaD_trXwRE&#34; title=&#34;Lexical Scanning in Go - Rob Pike&#34;&gt;Rob Pike’s talk on lexical scanning&lt;/a&gt;.
Instead of a loop where we get the next bit of text to examine and restore the
state of the processing by examing a number of state variables, we instead
have a loop where a function is called that returns the next function to be
called. In other words, a function is called which does a bit of processing of
the text, advancing the pointer or consuming from a stream, maybe emitting
some tokens, and then returns to the caller the function that should proceed
from where the returning function just left off. For instance, we just scanned
a category, which means we know we are ready to scan a film, so call the film
scan function. That next function can just carry on its processing without any
state-checking preliminaries. The loop of our system therefore is very
concise, just calling functions and getting the next one to call the
subsequent time around. Roughly:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def run():
    state = start_state
    while state:
        state = state()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When we are done processing input, say, EOF is reached, the state function
currently executing can return &lt;code&gt;None&lt;/code&gt; to the caller, which will end the while
loop and shut down the machine.&lt;/p&gt;

&lt;p&gt;The advantage to the programmer is that instead of building up a complicated
switch of control to determine what state our machine is in, we simply write
functions that proceed naturally from the last state, and then hand off
control to the subsequent function. It’s clean and helps keep the complexity
of the system manageable. Any time you can reduce the number of control flow
statements and replace them with simple functions is a win in my book.&lt;/p&gt;

&lt;p&gt;So back to the Oscars. This year, I opened the &lt;a href=&#34;http://cdn.media.oscar.abc.com/media/2013/pdf/2013/nominees.pdf&#34;&gt;official nominee list&lt;/a&gt; from
the Academy’s site, a PDF. I selected the text, copied and pasted it into a
text document. The only manual editing I did was to add a blank line between
each group of nominees by category, and I also joined lines in categories like
Music (Original Song) where the title of the song and the name of the composer
is split across multiple lines—these were quick changes that simplified the
scanning logic.&lt;/p&gt;

&lt;p&gt;There are three state functions in my program, one for each of category, film,
and name (or list of names):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def lex_category(lexer):
    lexer.emit(CATEGORY, title(getline()))
    return lex_film

def lex_film(lexer):
    line = getline()
    if line == &amp;#39;&amp;#39;:
        lexer.emit(BLANK, &amp;#39;&amp;#39;)
        return lex_category
    elif line is None: # EOF, shut down lex machine
        return None
    lexer.emit(FILM, title(line))
    return lex_names

def lex_names(lexer):
    lexer.emit(NAMES, title(getline()))
    return lex_film
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(&lt;code&gt;title()&lt;/code&gt; handles some odd case formatting in the source text by converting
strings to title case.)&lt;/p&gt;

&lt;p&gt;&lt;code&gt;lex_film&lt;/code&gt; is the most complex, having to handle the possibilities of a blank
line, meaning we’re moving on to the next category, EOF, which shuts down
scanning, and the film itself. But in all cases we merely return the next
state function to called (or &lt;code&gt;None&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Admittedly, this is more sophistication than normally appears in my yearly
nominee list parsing. But I have to say that I was able to write the program
in about the same amount of time, found it ran correctly the first time, and
was actually kind of fun to do. And while this was a silly example, you can
start to see the power you can get from this approach when lexing different
kinds of input with more and more complex tokens. When you lift the flow of
control up a level and let your functions focus on the task at hand, the
result I think is a more elegant and more obviously correct program.&lt;/p&gt;

&lt;p&gt;The script and input text are &lt;a href=&#34;https://gist.github.com/4507999&#34;&gt;here&lt;/a&gt;, and the output list of nominees is
&lt;a href=&#34;https://docs.google.com/spreadsheet/ccc?key=0AviXLd8uXec3dHRtenJGcUs5aTBXUEY4cWs2WHNpS3c#gid=0&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>The Election Day Advent Calendar</title>
    <link href="/blog/2012/08/the-election-day-advent-calendar.html" />
    <id>http://pauladamsmith.com/blog/2012/08/the-election-day-advent-calendar.html</id>
    <updated>2012-08-07T04:20:00Z</updated>
    <content type="html">&lt;p style=&#34;float: right; margin: 0 0 10px 10px&#34;&gt;&lt;a href=&#34;http://www.kickstarter.com/projects/electioncalendar/the-election-day-advent-calendar&#34; title=&#34;The Kickstarter for the Election Day Advent Calendar&#34;&gt;&lt;img src=&#34;/images/edac2012.jpg&#34; alt=&#34;The Election Day Advent Calendar&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;My friend Ben Helphand and I started scheming in 2005 on a way to
celebrate the right to vote. We believed that a great nation deserves
great civic rituals, and the election season was the natural time for
such a ritual. What we came up with was &lt;a href=&#34;http://electioncalendar.net/2006/&#34;&gt;the Election Day Advent
Calendar for 2006&lt;/a&gt;. The idea was simple: inspired by the
traditional Advent calendar for Christmas, our calendar let you open a
door a day through Election Day, and reveal interesting facts and quotes
about our democracy and the history of voting. It would be non-partisan
but not scrubbed of parties, a reflection on the many events and people
that comprise the story of the franchise.&lt;/p&gt;

&lt;p&gt;We wanted to make a real calendar that you could buy and hang up.
Neither of us had any experience in the print production world, and we
both had day jobs in different fields. We had to figure out all the
steps of the process, from hiring an artist to forming an LLC to sales
and shipping and marketing. We put up our own money to fund it, and
hoped to sell enough just to break even.&lt;/p&gt;

&lt;p&gt;The response to that first Calendar was strong, and told us there was a
desire for such products that let us all celebrate the shared history of
American politics. People were delighted by the concept; when they saw
it, they got it. We heard from teachers that it made a great part of
their back-to-school civics lesson plans, and from many people who said
they gave it out as a gift to a politics-nerd friend or colleague.&lt;/p&gt;

&lt;p&gt;After repeating the process in &lt;a href=&#34;http://electioncalendar.net/2008/&#34;&gt;2008&lt;/a&gt;, we took a break in 2010 but
didn’t give up on the core idea. What we needed, though, was a different
approach to paying for production—it was just too risky to put up our
own money time and again. In the meantime, along came Kickstarter. It’s
perfect for a project like ours.&lt;/p&gt;

&lt;p&gt;As I write, there are 6 days remaining in our campaign, and we’re a
little more than half way to our goal. If you’re reading these words and
like what I’ve described, &lt;strong&gt;&lt;a href=&#34;http://www.kickstarter.com/projects/electioncalendar/the-election-day-advent-calendar&#34;&gt;please support our Kickstarter&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Our motto with Gerrymander, the small company Ben and I founded for the
Calendar, is “Make Small Plans”. It’s a gently mocking riff on Daniel
Burnham’s famous and possibly apocryphal &lt;a href=&#34;http://www.ontko.com/pub/rayo/burnham.html&#34;&gt;quote&lt;/a&gt;. Our republic,
as great and large as it is, is still composed of the small democratic
actions of all of us, and those things require care and feeding, and
they ask to be done well. If the Election Day Advent Calendar provides
civic nourishment in any way, then it will have been a success.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Interview: Mike Migurski</title>
    <link href="/blog/2012/03/migurski.html" />
    <id>http://pauladamsmith.com/blog/2012/03/migurski.html</id>
    <updated>2012-03-30T03:45:00Z</updated>
    <content type="html">&lt;style&gt;
  dt {
    float: left;
    font-weight: bold;
    margin-right: 0.25em;
  }
  dt:after {
    content: &#34;:&#34;;
  }
  dd {
    margin: 0 0 1em 0;
    padding: 0;
  }
  .q + dd {
    font-weight: bold;
  }
  #map {
    height: 350px;
    width: 100%;
    border: none;
  }
&lt;/style&gt;
&lt;iframe id=&#34;map&#34; src=
&#34;http://maps.stamen.com/watercolor/embed#13/40.4404/-79.9928&#34;
name=&#34;map&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;a href=&#34;http://mike.teczno.com/&#34;&gt;Mike Migurski&lt;/a&gt; is partner
and Director of Technology at &lt;a href=
&#34;http://stamen.com&#34;&gt;Stamen&lt;/a&gt; design studio in San Francisco. He
and his Stamen colleagues recently unveiled a new site, &lt;a href=
&#34;http://maps.stamen.com/&#34;&gt;maps.stamen.com&lt;/a&gt;, that included
three custom-rendered maps using data from &lt;a href=
&#34;http://openstreetmap.org/&#34;&gt;OpenStreetMap&lt;/a&gt;, including a
remarkable one that, despite being generated by algorithm from
precise vector data, resembled a hand-made watercolor
painting.&lt;/p&gt;

&lt;dl&gt;
  &lt;dt class=&#34;q&#34;&gt;Paul Smith&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;You’ve blogged about the process of creating the Terrain
    map tiles, and the code for the Toner tiles is open-sourced and on
    GitHub, but the Watercolor tiles seemed to come out of the
    blue. Can you talk about what inspired them, how you and your
    Stamen colleagues went about designing them, and what uses
    you imagined for them?&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt&gt;Mike Migurski&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;The main use we imagined for them was that they would look
    really, really good, and unlike anything else being done with
    online maps. The process started with Eric Rodenbeck and Zach
    Watson kicking around some ideas on simulating the appearance
    of watercolor painting algorithmically, and expanded to
    include &lt;a href=
    &#34;http://www.flickr.com/photos/sensescape/sets/72157629121886440/&#34;&gt;
    Geraldine Sarmiento’s hand-done textures&lt;/a&gt;.&lt;/p&gt;

    &lt;p&gt;We saw an amazing reaction to &lt;a href=
    &#34;http://www.20x200.com/blog/2011/11/aaron-straup-copes-prettymaps-across-the-us-of-a.html&#34;&gt;
    Aaron Straup Cope’s work on Prettymaps&lt;/a&gt; last year, and
    more generally a lot of interest in map-based art, so it
    seemed natural to try something that was algorithmic and at
    the same time had the appearance of being made by hand. I
    figured people would swallow their tongues in surprise when
    they saw these, especially when we switched from image search
    results for painted textures to pictures we were making
    ourselves for the project.&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt class=&#34;q&#34;&gt;PS&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;Making maps these days is often about mechanizing the
    conversion of vector data, like from &lt;abbr title=
    &#34;OpenStreetMap&#34;&gt;OSM&lt;/abbr&gt;, into raster images, which has
    meant perfecting our ability to layer and style points,
    lines, and polygons in very precise ways. I think what
    captivated people about the Watercolor tiles is that they
    seemed to color outside the lines, so to speak, in a way that
    didn’t seem possible with current tools. What tools did you
    use to generate the tiles, and how did you engineer them in a
    way to achieve this effect? Did you have to write new
    software to aid you?&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt&gt;MM&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;We did write new software, yes. We’ve been exploring ways
    to make &lt;abbr title=&#34;OpenStreetMap&#34;&gt;OSM&lt;/abbr&gt; data more
    usable by cartographers over the past few years, and that’s
    largely been expressed in code designed to make large data
    sets easier for cartographers (and ourselves) to deal
    with:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;a href=
        &#34;http://mike.teczno.com/notes/cascadenik.html&#34;&gt;Cascadenik&lt;/a&gt;
        was the first widespread application of CSS-like style
        rules in this area, since developed and expanded by
        Development Seed in Carto.
      &lt;/li&gt;

      &lt;li&gt;
        &lt;a href=&#34;http://tilestache.org/&#34;&gt;TileStache&lt;/a&gt; is a
        tile-generation server built to deploy custom
        visualization and rendering code.
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;aside class=&#34;pull&#34;&gt;I figured people would swallow their tongues in
    suprise when they saw these.&lt;/aside&gt;

    &lt;p&gt;I’ve been talking about this stuff for a few years, and
    the release of &lt;a href=
    &#34;http://maps.stamen.com/&#34;&gt;maps.stamen.com&lt;/a&gt; is intended to
    be a showcase that helps shift expectations of what good,
    online cartography should look like. It hasn’t been long
    since using a solid anti-aliased vector-rendering library
    like Mapnik was enough to make a strong showing against
    commercial map providers, but now we’re starting to establish
    new ideas around texture, color and labeling that require a
    layer of additional work beyond Mapnik.&lt;/p&gt;

    &lt;p&gt;Watercolor itself is a custom Provider to TileStache,
    feeding on simple Mapnik output and adding techniques like
    gaussian blur and Perlin noise as well as high-quality scans
    of actual water color paintings to achieve its effect. The
    development of watercolor had a number of stages, beginning
    with Zach’s experiments in noise and texture, Eric and George
    Oates’s encouragement, Geraldine’s addition of color and
    texture, and proceeding to my work with Jeff Easter and
    Nathaniel Kelso to improve performance until the code was
    good enough for public launch. The visual appearance is
    ridiculously CPU-hungry compared to your typical vector
    cartography, so we’ve got a few virtual servers on the moon
    doing the live calculation of new maps in new places.&lt;/p&gt;

    &lt;p&gt;Without diving too deeply into the actual watercolor code,
    we owe a lot to &lt;a href=
    &#34;http://www.mprove.de/script/90/KPT/index.html&#34;&gt;Kai Krause’s
    Algorithmic Painting&lt;/a&gt; ideas from almost 20 years ago.&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt class=&#34;q&#34;&gt;PS&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;I see that Stamen is releasing these tiles under a
    Creative Commons license as part of a Knight
    Foundation-funded project centered around data visualization
    and cities. City governments are historically big users of
    enterprise GIS software like ArcGIS; are there any lessons
    here for government or citizens, or does the work,
    specifically the Watercolor tiles, speak for themselves as
    art?&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt&gt;MM&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;I think the latter: watercolor is basically art. Of course
    there’s an undeniable political component to the
    OpenStreetMap data it’s made of. In the U.S. &lt;abbr title=
    &#34;OpenStreetMap&#34;&gt;OSM&lt;/abbr&gt; Foundation we’re interested in
    ways to get government at various levels to participate
    custodians of &lt;abbr title=&#34;OpenStreetMap&#34;&gt;OSM&lt;/abbr&gt;, and I
    think open data and enterprise GIS software can cooperate.
    Esri puts a lot of effort into OpenStreetMap-related efforts,
    and I’m not expecting to see their involvement in city
    governments wane.&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt class=&#34;q&#34;&gt;PS&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;Can you expand a bit on the interplay of vector data,
    raster data, and image filters to produce the tiles? What
    sort of preliminary work did you need to do to get the
    components parts to fall into place? Do you see patterns
    emerging and opportunities for abstraction for future artist
    map tools?&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt&gt;MM&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;I probably can’t explain as well as &lt;a href=
    &#34;http://content.stamen.com/watercolor_process&#34;&gt;Zach just
    did&lt;/a&gt;.&lt;/p&gt;

    &lt;p&gt;As a bystander to this part of the process, it seemed like
    the actual simulation technique took shape in just a few
    days, based on Zach’s existing experience with image
    manipulation in Python. The real work came in the
    knob-twiddling, or as we call it internally, “spring tuning”
    (based on our experience with the Digg Swarm project from
    2007). &lt;a href=
    &#34;http://content.stamen.com/files/noise%20v%20threshold.jpg&#34;&gt;Here,
    for example, is one of Zach’s own parameter views&lt;/a&gt; that he
    used to tune noise thresholds for the ground texture.&lt;/p&gt;

    &lt;aside class=&#34;pull&#34;&gt;It’s intended to shift expectations of what
    good, online cartography should look like.&lt;/aside&gt;

    &lt;p&gt;I think the pattern that’s emerging for me is the raw
    labor-intensivity of this kind of work, the
    parameter-tweaking in a space of possible outcomes that
    results in something that looks right and feels exciting.
    Once the basic structure of noise, blur and threshold is in
    place, all you can really do is watch carefully as you
    repeatedly try combinations until something clicks.&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt class=&#34;q&#34;&gt;PS&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;We’re usually talking about algorithms and stylesheets
    when we talk about web maps. Traditional cartographers often
    exercise artistic license over data streams as well—for
    example, manually but subtlety tweaking the curve of a road
    so that it reflects a shared or colloquial understanding of
    its location rather than it’s literal location. And then
    there are the more abstract but functional examples like
    subway system maps, whose stops and lines are not intended to
    be scale representations of their real-world counterparts. Do
    you see it possible for automated cartography to produce maps
    like these? What techniques would we need to develop (for
    example, a “hints” file ala typography for manually
    overriding certain points)?&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt&gt;MM&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;I’ve not yet seen an attempt at automating this kind of
    cartography which has resulted in a satisfying outcome, but
    it’s still the subject of many PhD theses, so maybe it’s just
    too early. I suspect that we’ll end up seeing is a companion
    project to &lt;abbr title=&#34;OpenStreetMap&#34;&gt;OSM&lt;/abbr&gt; where human
    make decisions about how things should be shown and
    contribute those to a free and open data source. Everything
    is still so manual in this world, and the subject of most
    maps doesn’t move around all that much, so you can really
    apply a human eye to get it right. Even with the watercolors,
    we had to do a lot of manual work to ensure that the
    1024×1024 watercolor texture blended cleanly and the various
    road sizes looked correct at each zoom level.&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt class=&#34;q&#34;&gt;PS&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;Regarding deployment. One of the challenges of producing
    raster map tiles for the web is the amount of storage and CPU
    time it takes to generate them; I notice that Stamen’s tiles
    are available down to zoom 18, which for a worldwide set
    means there are millions and millions of individual PNG
    files. To a degree CPU time can be amortized over the life of
    the project if you’re using a tile server to dynamically
    generate and cache tiles as users first request them, but
    even with commodity storage like S3, you’re talking about
    hundreds of GB or more. Are there knock-on challenges this
    presents for deployment and maintenance? Is there a
    sustainability plan for maps.stamen.com with regard to
    storage and bandwidth costs, or is Stamen as a company just
    going to eat those costs to provide this resource?&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt&gt;MM&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;We’re using a mix of physical and virtual machines for
    each of the tile sets we just released, blending the
    strengths and weaknesses of each. The CPU-intensive rendering
    of watercolor maps is done on Amazon EC2 where we can invoke
    extra machines as necessary, but the PostGIS, Mapnik and
    cache storage parts are all living on an actual server in a
    colocation facility. We decided last year to invest in
    physical hardware to take advantage of the high random-access
    speed of solid state disks, which make it possible to serve
    the entire OpenStreetMap planet database without incurring
    the overhead of Amazon’s terribly slow I/O speeds.&lt;/p&gt;

    &lt;p&gt;Fortunately, the back-end of this project is used to drive
    a lot of our other work, so we’re folding the cost into a
    series of different engagements that all use different
    components of the map tiles.&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt class=&#34;q&#34;&gt;PS&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;We appear to be reaching a tipping point where rolling
    your own custom map stack seems not only practical but
    desirable for many applications. What is your reaction to the
    embrace of &lt;abbr title=&#34;OpenStreetMap&#34;&gt;OSM&lt;/abbr&gt; by
    prominent technology companies, and the emergence of
    designer-friendly tools like TileMill? Do we have everything
    we need for most designers and developers to create the map
    experiences they want to provide? What tools would you’d like
    to see built; what data sets made available?&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt&gt;MM&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;Honestly, I’m completely thrilled. With this year’s
    high-profile addition of Foursquare and Apple to the
    OpenStreetMap community, I’m looking forward to seeing what
    new artists and designers decide to do with maps—I can’t
    wait for the U.S. State Of The Map conference this
    autumn.&lt;/p&gt;

    &lt;aside class=&#34;pull&#34;&gt;Bitmap tiles have an equilibrium of performance,
    size and design that I don’t think will be disturbed any time
    soon.&lt;/aside&gt;

    &lt;p&gt;I think there are two issues that current tools don’t
    address well enough, and I’m excited to be working on them
    both: medium-scale data for counties and towns, and more
    options for bitmap filtering and output. I want Photoshop and
    Illustrator in the sky, essentially, and tools like TileMill
    help expose places where we need to be doing more with data
    before rendering and more with pixels after rendering.&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt class=&#34;q&#34;&gt;PS&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;Elaborate on what you mean by “medium-scale data”, and how
    it would improve map-making.&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt&gt;MM&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;My colleague Nathaniel Kelso runs a project called
    &lt;a href=&#34;http://www.naturalearthdata.com/downloads/&#34;&gt;Natural
    Earth Data&lt;/a&gt;, which offers global vector data at three
    different scales, optimized for rendering images of large
    regions, countries and continents, up to about zoom=9 if you
    think in terms of web slippy maps. OpenStreetMap meanwhile
    offers small-scale data down to the level of individual
    carriageways on major streets. There’s a gap between these
    two data sets where &lt;abbr title=&#34;OpenStreetMap&#34;&gt;OSM&lt;/abbr&gt; is
    too detailed and Natural Earth is not detailed enough, so on
    many renderings you get bizarre selections of town names to
    render, doubled-up street names, or no global context.&lt;/p&gt;

    &lt;p&gt;The &lt;a href=&#34;http://maps.stamen.com/#terrain&#34;&gt;Terrain
    layer on maps.stamen.com&lt;/a&gt; is a vehicle for exploring a few
    avenues through this problem: feature generalization for
    route shields and large street names using the &lt;a href=
    &#34;http://github.com/migurski/Skeletron&#34;&gt;Skeletron&lt;/a&gt; library,
    simulated annealing for smarter label placement with &lt;a href=
    &#34;https://github.com/migurski/Dymo&#34;&gt;Dymo&lt;/a&gt;, and &lt;a href=
    &#34;https://github.com/migurski/DEM-Tools&#34;&gt;cross-blending of
    raster data sets for ground cover and hill shading&lt;/a&gt;. We’re
    doing a lot of work to make &lt;abbr title=
    &#34;OpenStreetMap&#34;&gt;OSM&lt;/abbr&gt; data better for rendering, and
    releasing all the component parts as software for processing
    data sets.&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt class=&#34;q&#34;&gt;PS&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;Do you see Mapnik as the appropriate place to grow the
    bitmap filtering and output functions?&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt&gt;MM&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;As far as Mapnik’s role in all this, I think it’s the
    single best place to do vector rendering, but I’m looking
    elsewhere for filtering and output. I prefer to use tools
    that are specialized for individual tasks, so we use the
    pixel output of Mapnik as a source for Python-based pixel
    manipulation code, often implemented in libraries like NumPy
    that offer rapid manipulation of bitmap arrays.&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt class=&#34;q&#34;&gt;PS&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;It sounds like you’re hinting at a new program for this
    kind of manipulation. You and Stamen have contributed a great
    deal of open-source map making software over the years; will
    the code that created Watercolors be released?&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt&gt;MM&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;I hope I’m not sounding too coy; there’s not any secret
    piece of software running the show, just a set of well-known
    techniques in a new arrangement. I’ve released everything
    I’ve ever started, but my role in Watercolor was more about
    taking something that already worked and making it work a
    little better so more people could see it.&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt class=&#34;q&#34;&gt;PS&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;Most web maps are a pre-baked set of raster images
    assembled in the viewport of the browser—do you see this
    fundamental arrangement changing in the foreseeable future?
    With WebGL and HTML5 Canvas, are we ready to composite maps
    client-side with servers pushing vectors data over the wire?
    What tradeoffs are there here? Do you think you could do
    Watercolors entirely in the browser anytime soon?&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt&gt;MM&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;I don’t think we’re far off, though there are a few
    impediments still in the way. Much of the core functionality
    of Watercolor’s bitmap effects can be implemented entirely on
    a GPU in a WebGL fragment shader, so there’s no reason that
    we couldn’t build it that way. As far as shipping vectors,
    I’d love to see it happen. It’s actually not unrelated to the
    work we’re doing with differently-scaled data: you want your
    underlying data to be at the “right” level of complexity, and
    that generally means modifying it in some way, by dropping
    extra points, scrunching narrow polygons into lines, making
    small things disappear, and lumping groups of things together
    into single blobs. If we can figure out a better way to
    simplify on the fly, then complete client-side rendering
    could be a reality. Of course, Google has already done a lot
    of this themselves, but it’s different when the open source
    community does it and shares the results and the
    research.&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt class=&#34;q&#34;&gt;PS&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;Simplification of lines and polygons is a common need for
    mapping projects, not only for display but also for
    interaction—clients can only handle so many points during the
    refresh cycle. It can be a big challenge for workflows to
    simplify vectors, and also to preserve topology, the
    relationship between shapes—our model is of disconnected
    points, lines, and polygons. That’s one reason I’m excited
    about topology as a type coming in PostGIS 2.0. What
    simplification techniques should we be using and exploring,
    both on the algorithm side as well as the workflow side?&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt&gt;MM&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;I spent about two years (off and on) researching line
    generalization. Skeletron went through three iterations,
    starting with a port of the straight skeleton technique from
    &lt;a href=&#34;http://teczno.com/s/vkp&#34;&gt;Tom Kelly’s
    description&lt;/a&gt;, to a collaboration with Schuyler Erle
    binding to the &lt;a href=&#34;http://teczno.com/s/vlv&#34;&gt;CGAL library
    from Python&lt;/a&gt;, finally settling on an application of the
    &lt;a href=&#34;http://teczno.com/s/2trs&#34;&gt;Voronoi diagram first
    published by Esri&lt;/a&gt; in 1996. All the way through, I kept
    thinking that there just had to be an easier way to make
    simple lines out of complex ones, and why wasn’t I finding
    code to do it for me? The simplification techniques we should
    explore are all known and published, but exist largely as
    plugins for systems like ArcGIS, instead of chainable tools
    in the Unix style. PostGIS 2.0 is going to help in a big way,
    and I hope that some of that effort migrates out to tools for
    managing workflows around flat files.&lt;/p&gt;

    &lt;aside class=&#34;pull&#34;&gt;All you can really do is watch carefully as you
    repeatedly try combinations until something clicks.&lt;/aside&gt;

    &lt;p&gt;If all goes well, it should start to make sense to ship
    vectors over the wire and render them on the client.&lt;/p&gt;

    &lt;p&gt;It’s an interesting question to me whether client-side
    rendering really something we want to aim for, though. Bitmap
    tiles have an equilibrium of performance, size and design
    that I don’t think will be disturbed any time soon. I’m
    learning what I can about GPUs to be ready when the day
    comes, but in the meantime most of my focus is on developing
    workflows for data. &lt;abbr title=&#34;OpenStreetMap&#34;&gt;OSM&lt;/abbr&gt;
    extracts are one aspect of this; simplifying &lt;abbr title=
    &#34;OpenStreetMap&#34;&gt;OSM&lt;/abbr&gt; data and standing up fresh new
    rendering databases from source data is another. The
    scaffolding that makes life easiest is a combination of bias
    toward flat files and Postel’s Law: don’t screw around with
    “seamless” servers, publish data flat using old file formats,
    and only invent brand new things when they’re needed.
    Spherical Mercator slippy map tiles are fast becoming one of
    those well-understood old file formats, so this is what we’ve
    aimed for with Watercolor; otherwise, how could we have a
    section on maps.stamen.com showing how to use the imagery in
    your own applications?&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt class=&#34;q&#34;&gt;PS&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;That’s interesting—so are you generally fine with the way
    Spherical Mercator has colonized the web map world? It seems
    to have been a decision purely about tradeoffs: what
    projection “works” for world-wide maps all the way down to
    zoom level 22, or however detailed Google Maps gets these
    days. But clearly choice of projection is a never-ending
    debate. Is Spherical Mercator an acceptable enough
    constraint, from your perspective, within which to do the
    kind of artistic expression Stamen does, and maintain
    interoperability?&lt;/p&gt;
  &lt;/dd&gt;

  &lt;dt&gt;MM&lt;/dt&gt;

  &lt;dd&gt;
    &lt;p&gt;Oh yeah, I’m absolutely fine with it. It’s one of those
    “assume a spherical chicken” engineering solutions that
    actually leads to so much follow-on innovation that in
    retrospect I’m glad no one was letting the cartographers
    drive at Google in 2005. Projections only matter when you’re
    looking at large areas, and that’s really not the case when
    you’re searching for driving directions or checking out the
    neighborhood where you spent elementary school. It’s
    interesting to me, though, that the typical fifty states map
    you see on every link bait infographic out there is based on
    a conic projection—it’s about what looks right for a genre of
    mapmaking, and in the case of slippy maps the Spherical
    Mercator is clearly the obvious choice.&lt;/p&gt;
  &lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;—&lt;a href=&#34;http://twitter.com/paulsmith&#34; title=&#34;Follow me on Twitter&#34;&gt;@paulsmith&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Going to work for Democrats</title>
    <link href="/blog/2011/09/dnc.html" />
    <id>http://pauladamsmith.com/blog/2011/09/dnc.html</id>
    <updated>2011-09-04T16:00:00Z</updated>
    <content type="html">&lt;p&gt;&lt;img src=&#34;/images/eb-to-dnc.png&#34; style=&#34;display: block; float: right; margin-left: 15px&#34;&gt;&lt;/p&gt;

&lt;p&gt;I have some exciting news: in a couple of weeks, I will be the new deputy
director of technology at the &lt;a href=&#34;http://www.democrats.org/&#34;&gt;Democratic National Committee&lt;/a&gt;. I’ll
be helping to create software that helps get Democrats elected. It’s
a great opportunity for me to do what I love for an organization that I
passionately support. I hope to help make the technology department there
top-notch in terms of software engineering practices, bringing what I’ve
learned from having helped develop, deploy, and grow &lt;a href=&#34;http://www.everyblock.com/&#34;&gt;EveryBlock&lt;/a&gt;. And
I’ll have some projects of my own, which will hopefully advance the
state of campaign software somewhat. It’s especially exciting, too, to
be able to collaborate with &lt;a href=&#34;http://twitter.com/harper&#34;&gt;friends&lt;/a&gt; &lt;a href=&#34;http://twitter.com/detour1999&#34;&gt;in&lt;/a&gt; &lt;a href=&#34;http://twitter.com/gabaug&#34;&gt;Chicago&lt;/a&gt;
&lt;a href=&#34;http://twitter.com/scottvdp&#34;&gt;working&lt;/a&gt; &lt;a href=&#34;http://twitter.com/aconbere&#34;&gt;on&lt;/a&gt; &lt;a href=&#34;http://www.barackobama.com/&#34;&gt;that one campaign&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I have been volunteering on political campaigns—federal, state, and
local races—for years, and have often lamented the state of campaign
software. It’s partly understandable, because campaigns tend to be
all-hands-on-deck, hair-on-fire affairs, where it’s hard to justify
long-range planning and software development, even if it might make
the lives of your staff, organizers and volunteers easier, since your
organization may not even exist for more than a few months. And campaigns
rarely have in-house software engineers, so opportunities to capture and
encode knowledge in the form of software, and explore new technologies,
are missed.&lt;/p&gt;

&lt;p&gt;Obama For America gets this—that’s why they’ve hired like a start-up
for this campaign cycle, recognizing that great software is a competitive
advantage and no longer an afterthought you contract out for. And the DNC
gets it, too, and that’s why I’m excited to join them. The chance to
help re-elect this president, restore Democratic majorities in Congress,
and also to help down-ballot Democrats across the country in this and
future campaign cycles is one I couldn’t pass up.&lt;/p&gt;

&lt;p&gt;I’ll be commuting to the DNC’s offices in Washington, D.C. from Baltimore
on a regular basis, though I’ll still be working from home a couple of days
each week, so that I won’t too miss much of &lt;a href=&#34;/blog/2011/08/maxine.html&#34;&gt;this kind of stuff&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It’s a bittersweet development, because I’ll be leaving EveryBlock,
which I helped found 4 years ago. With the success of the &lt;a href=&#34;http://blog.everyblock.com/2011/mar/21/redesign/&#34;&gt;recent
relaunch&lt;/a&gt;, though, I feel now is as good a time as any to step
away. The site is in great hands, and the response from users to the
new version has been enthusiastic. It couldn’t have a better home than
&lt;a href=&#34;http://www.msnbc.com/&#34;&gt;msnbc.com&lt;/a&gt;, who have provided great guidance and resources. I’m
thrilled with the success we’ve had and for how far it’s come, and
I’m confident that it will continue to be the best place on the internet
to help make your block a better place.&lt;/p&gt;

&lt;p&gt;I am particularly grateful for having worked with my great EveryBlock
colleagues. I’m humbled by them and their talents and work ethic. It
was a privilege to learn from them and improve my craft, however modestly,
by their examples.&lt;/p&gt;

&lt;p&gt;For now, I’m focusing on winding down at EveryBlock, and getting prepared
for a new commute (I’ll try to hack it with the MARC train and a bike)
and a campaign season now fully engaged. It’s a thrilling opportunity,
and I hope to make the most of it.&lt;/p&gt;

&lt;p&gt;—&lt;a href=&#34;http://twitter.com/paulsmith&#34;&gt;@paulsmith&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Maxine</title>
    <link href="/blog/2011/08/maxine.html" />
    <id>http://pauladamsmith.com/blog/2011/08/maxine.html</id>
    <updated>2011-08-27T20:10:00Z</updated>
    <content type="html">&lt;p&gt;&lt;a href=&#34;http://www.flickr.com/photos/psmith/6076865981/&#34; title=&#34;P1040879.jpg by pauladamsmith, on Flickr&#34;&gt;&lt;img src=&#34;http://farm7.static.flickr.com/6184/6076865981_a0c284c99a.jpg&#34; width=&#34;500&#34; height=&#34;334&#34; alt=&#34;P1040879.jpg&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;One month ago today, Maxine Mills Smith arrived in this world. Her mom gave
birth to her at Mercy Medical Center in Baltimore, Maryland, at six in the
morning. She was, and continues to be, a long and strong gal. She enjoys
being carried about and looking up at the changing scenery. Like her father,
she zonks out to the motion of a vehicle ride, be it stroller or car. She
has healthy lungs, likes to exercise them, and is a vocal chirper. She
has been respecting her mom and dad with 5 and 6-hour stretches of sleep
at night, but likes to mix it up from time to time and throw her mom some
napless curveballs. When she is awake, she is bright and alert. Given the
choice, she’d rather have some light assistance and try to stand and
monster march (I said she was strong) than squirm around on her belly like
a beetle.  Her binky is a frequent companion, and she is a champion eater
of mom’s milk.  The doctors and nurses at the pediatrician’s office
think she is doing fine, and give her an extra wink. We think she’s is
doing fine, too. Family love Maxine. Welcome home, my daughter.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Parsing and formatting date/time in Go</title>
    <link href="/blog/2011/05/go_time.html" />
    <id>http://pauladamsmith.com/blog/2011/05/go_time.html</id>
    <updated>2011-05-20T02:00:00Z</updated>
    <content type="html">&lt;p&gt;Go takes an interesting approach to &lt;a href=&#34;http://golang.org/pkg/time/&#34;&gt;parsing strings to time objects,
and formatting time objects as strings&lt;/a&gt;. Instead of using codes like
most languages to represent component parts of a date/time string
representation—like &lt;code&gt;%Y&lt;/code&gt; for a 4-digit year like “2011” or &lt;code&gt;%b&lt;/code&gt;
for an abbreviated month name like “Feb”—Go uses a mnemonic device:
there is a standard time, which is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Mon Jan 2 15:04:05 MST 2006  (MST is GMT-0700)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or put another way:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;01/02 03:04:05PM &amp;#39;06 -0700
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Instead of having to remember or lookup the traditional formatting codes for
functions like &lt;code&gt;strftime&lt;/code&gt;, you just count one-two-three-four and each place
in the standard time corresponds to a component of a date/time object (the
&lt;code&gt;Time&lt;/code&gt; type in Go): one for day of the month, two for the month, three for
the hour (in 12-hour time), four for the minutes, etc.&lt;/p&gt;

&lt;p&gt;The way you put this into action is by putting together the parts of
the standard time in a layout string that matches the format of either
the string representation you want to parse into a &lt;code&gt;Time&lt;/code&gt; object or the
opposite direction, when you want to generate a string representation from
an &lt;code&gt;Time&lt;/code&gt; object.&lt;/p&gt;

&lt;p&gt;Parsing:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;time&amp;quot;
)

func main() {
    value  := &amp;quot;Thu, 05/19/11, 10:47PM&amp;quot;
    // Writing down the way the standard time would look like formatted our way
    layout := &amp;quot;Mon, 01/02/06, 03:04PM&amp;quot;
    t, _ := time.Parse(layout, value)
    fmt.Println(t)
}

// =&amp;gt; &amp;quot;Thu May 19 22:47:00 +0000 2011&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Formatting:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;time&amp;quot;
)

func main() {
    t := time.SecondsToLocalTime(1305861602)
    t.ZoneOffset = -4*60*60
    fmt.Println(t.Format(&amp;quot;2006-01-02 15:04:05 -0700&amp;quot;))
}

// =&amp;gt; &amp;quot;2011-05-20 03:20:02 -0400&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are predefined constants in the &lt;code&gt;time&lt;/code&gt; package for common formats
such as RFCs 822 and 3339.&lt;/p&gt;

&lt;p&gt;The use of a mnemonic over obscure formatting codes I think reflects the
pragmatism of Go’s developers and their focus on creating a language
that makes its users more productive. While it is a break with tradition
to abandon &lt;code&gt;strftime&lt;/code&gt;-style formatting, they probably recognized that most
developers, no matter how experienced, reach for &lt;code&gt;man strftime&lt;/code&gt; or similar
documentation more often than not (hell, I have a mug with the codes printed
on it), and each time they do, it is an expensive context switch, in terms
of lost focus. Writing the standard time down the way yours looks may be
quirky, but it&amp;#39;s easy to recall, and it also happens to match the shape of
your time string, syntatically.&lt;/p&gt;

&lt;p&gt;It’s fascinating to see a language with the pedigree of Go—from the
minds of long-time C and Unix developers—toss aside certain old and
venerable ways of doing things. But it’s consistent with a language
that’s relatively small (in the good way of being comprehensible and
coherent), focused on efficiency, and careful in what it includes.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>A partial list of metaphors, visual and otherwise, anti-skeuomorphism zealots have to tackle to reach utopia</title>
    <link href="/blog/2011/04/floppy.html" />
    <id>http://pauladamsmith.com/blog/2011/04/floppy.html</id>
    <updated>2011-04-05T18:48:00Z</updated>
    <content type="html">&lt;p&gt;&lt;img src=&#34;/images/icons.jpg&#34; alt=&#34;icons&#34;&gt;
Motivated by irrational hatred and overstated claims of the continuing
utility of a 3.5&amp;quot; floppy disk icon to mean “save”:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Handset icon, meaning “phone,” on every iPhone, Android&lt;/li&gt;
&lt;li&gt;e&lt;strong&gt;mail&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Web&lt;strong&gt;site&lt;/strong&gt; (implies a physical place)&lt;/li&gt;
&lt;li&gt;Wrench or cog icon, meaning “settings” or “preferences”&lt;/li&gt;
&lt;li&gt;Shopping cart icon&lt;/li&gt;
&lt;li&gt;Clock with analog hands, meaning “time” or time-centric application&lt;/li&gt;
&lt;li&gt;Incandescent light bulb, several meanings, including energy settings&lt;/li&gt;
&lt;li&gt;Bound, paper book, meaning “book”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Of course there are many, many anachronistic interface elements and metaphors
in the world, and &lt;a href=&#34;/blog/2010/10/old_things.html&#34;&gt;we get by okay&lt;/a&gt;. Mainly,
this is because new generations don’t suddenly appear next to us and
start using our computers without any foreknowledge of the metaphorical
items. They spend time learning with the old fogeys for whom information was
sometimes stored inside square pieces of plastic and metal. This overlap is
necessary in general because knowledge about tools is not encoded in our
genetic material. All understanding of &lt;em&gt;use&lt;/em&gt; is part of a multi-layered
strata of culture, experience, and relationships.&lt;/p&gt;

&lt;p&gt;The real problem anti-floppy-disk people have is explaining a harm,
specifically, a harm that matters. Often when we think of user interface
improvements that matter, we think of examples like improving medical devices
to reduce user error, changes that literally save people’s lives. Or
tweaks to software that improve user efficiency and productivity, saving
money. It’s hard to conceive of what might be improved by finding a better
metaphor for “save” other than some designers’ personal sensibilities.&lt;/p&gt;
</content>
  </entry>
  </feed>